<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic
  PUBLIC "-//OASIS//DTD DITA Topic//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/topic.dtd" ><topic xml:lang="en-us" id="topic9683">
<title>HP Helion OpenStack® 1.1: Ceph Automated Installation</title>
<prolog>
<metadata>
<othermeta name="layout" content="default"/>
<othermeta name="product-version" content="HP Helion OpenStack"/>
<othermeta name="product-version" content="HP Helion OpenStack 1.1"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Storage Architect"/>
<othermeta name="role" content="Storage Administrator"/>
<othermeta name="role" content="Storage Engineer"/>
<othermeta name="role" content="Service Developer"/>
<othermeta name="role" content="Cloud Administrator"/>
<othermeta name="role" content="Application Developer"/>
<othermeta name="role" content="Binamra S, Paul F"/>
<othermeta name="product-version1" content="HP Helion OpenStack"/>
<othermeta name="product-version2" content="HP Helion OpenStack 1.1"/>
</metadata>
</prolog>
<body>
<p>
<!--PUBLISHED-->
 <!--./commercial/GA1/ceph/1.1commercial.ceph-automated-install.md-->
 <!--permalink: /helion/openstack/1.1/commercial.ceph-automated-install/--></p>
<!--
<p style="font-size: small;"> <a href="/helion/openstack/1.1/install-beta/kvm/">&#9664; PREV</a> | <a href="/helion/openstack/1.1/install-beta-overview/">&#9650; UP</a> | <a href="/helion/openstack/1.1/install-beta/esx/">NEXT &#9654;</a> </p>
-->
<p>The installation workflow of HP Helion OpenStack®1.1 is shown in the following diagram.</p>
<p>
  <image href="../../../media/ceph-installation-flow.png"/>
</p>
<p>The next diagram shows the logical architecture of Ceph.</p>
<p>
  <image href="../../../media/ceph-logical-architecture.png"/>
</p>
<p>The Ceph-automation tools are bundled into two tarball files:</p>
<ul>
<li>
<codeph>cephsetup.tar</codeph>
</li>
<li>
<codeph>cephconfiguration.tar</codeph>
</li>
</ul>
<p>You get these files from <xref href="https://helion.hpwsportal.com" scope="external" format="html" >https://helion.hpwsportal.com</xref>
</p>
<section>
<title>The <codeph>cephsetup.tar</codeph> file</title>
<p>You must downloaded and copy the <codeph>cephsetup.tar</codeph> file to the Helion seed VM. The cephsetup.tar file is bundled with the following packages:</p>
<ul>
<li>
<codeph>images</codeph>
</li>
<li>
<codeph>helion-patch</codeph>
</li>
<li>
<codeph>node-provisioner</codeph>
</li>
</ul>
</section>
<section>
<required-cleanup>
<title>
<codeph>Images</codeph> package</title>
<p>Ceph-automation provides two types of images:</p>
<ul>
<li>
<codeph>ceph-admin-image</codeph> includes all the packages required to run configuration scripts. This image are used on Ceph admin node.</li>
<li>
<p>
<codeph>ceph-cluster-image</codeph> includes all the packages for setting up <codeph>ceph-monitor</codeph>, <codeph>ceph-osd</codeph> and <codeph>ceph-rados</codeph> Gateway.</p>

<p>Except for the Ceph admin node, the <codeph>ceph-cluster-image</codeph> is shared across all the nodes participating in cluster.</p>
</li>
</ul>
</required-cleanup>
</section>
<section>
<required-cleanup>
<title>
<codeph>helion-patch</codeph> package</title>
<p>The <codeph>helion-patch</codeph> includes:</p>
<ul>
<li>
<codeph>hp_ced_check_deployed_images</codeph>
</li>
<li>
<codeph>hp_ced_pre_update_checks.sh</codeph>
</li>
<li>
<codeph>register-cephnodes.sh</codeph> </li>
</ul>
</required-cleanup>
</section>
<section>
<required-cleanup>
<title>
<codeph>node-provisioner</codeph> package</title>
<p>To deploy these images using the provisioning tool that runs from the seed VM:</p>
<ol>
<li>
<p>Login to the Helion seed VM.</p>

<codeblock>
<codeph>ssh root @ &lt;seed IP address&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Create a directory on any location</p>

<codeblock>
<codeph>mkdir /helion-ceph dir 
</codeph>
</codeblock>
</li>
<li>
<p>Download the tar file from <xref href="https://helion.hpwsportal.com" scope="external" format="html" >https://helion.hpwsportal.com</xref>
</p>
</li>
<li>Untar and copy the downloaded file to the <codeph>/helion-ceph</codeph> folder on the seed VM.</li>
</ol>
<p>The following directories are created under <codeph>/helion-ceph</codeph> on  the seed VM</p>
<ul>
<li>
<codeph>images</codeph>
</li>
<li>
<codeph>node-provisioner</codeph>
</li>
</ul>
<ol>
<li>
<p>To view the contents of the <codeph>/helion-ceph/images</codeph> directory from the seed VM, enter (for example):</p>

<codeblock>
<codeph>root@hLinux:~/helion-ceph/images# ls
bm-deploy-kernel
bm-deploy-ramdisk
overcloud-ceph-admin-initrd
overcloud-ceph-admin-vmlinuz
overcloud-ceph-admin.qcow2
overcloud-ceph-cluster-initrd
overcloud-ceph-cluster-vmlinuz
overcloud-ceph-cluster.qcow2
</codeph>
</codeblock>
</li>
<li>
<p>To view the contents in the <codeph>/helion-ceph/node-provisioner</codeph>directory from the seed VM, enter (for example):</p>

<codeblock>
<codeph>/helion-ceph/node-provisioner# ls
baremetal.csv
cleanup.py
common.py
orchestration.json
orchestration.py    

/helion-ceph/node-provisioner/server# ls
app.wsgi
bottle.py
httpd.conf
server.json
 server.py

/helion-ceph/node-provisioner/client# ls
baremetal.csv
common.py
orchestration.json
orchestration.py
</codeph>
</codeblock>
</li>
<li>
<p>Modify <codeph>/server/server.json</codeph> to include OpenStack credentials from the Undercloud (<codeph>stackrc</codeph>), network, and keypair that you want to use. The following is a sample of <codeph>server.json</codeph>:</p>

<codeblock>
<codeph>root@hLinux:~/helion-ceph/node-provisioner/server# cat server.json
{
"authentication": {
   "HOST": "127.0.0.1",
   "PORT": "8085",
   "OS_VERSION": "2",
   "OS_USER": "admin",
   "OS_PASSWORD": "de44ff17c309c7b2a74465104cd59728b121dd60",
   "OS_TENANT_NAME": "admin",
   "OS_AUTH_URL": "http://&lt;undercloud IP&gt;:5000/v2.0",
   "keypair": "cephadmin",
   "netid": "823ad730-6c69-42e6-b455-8a8173ceae81"
    }
}
</codeph>
</codeblock>
</li>
</ol>
<p>where:</p>
<p>
<b>HOST</b> is the local machine IP which will be your seed IP.</p>
<p>
<b>PORT</b> is 8085 which can be used as a service port.</p>
<p>
<b>OS_VERSION</b> is the OpenStack API version. Should be set to "2".</p>
<p>
<b>OS_USER</b> is supplied from the Undercloud <codeph>stackrc</codeph> file. For example: <b>admin"</b>.</p>
<p>
<b>OS_PASSWORD</b> -is supplied from the Undercloud <codeph>stackrc</codeph> file. For example: "de44ff17c309c7b2a74465104cd59728b121dd60".</p>
<p>
<b>OS    _TENANT_NAME"</b> is supplied from the Undercloud <codeph>stackrc</codeph> file.</p>
<p>
<b>OS    _AUTH_URL"</b> is the Undercloud IP. For example: "http://192.168.51.23:5000/v2.0".</p>
<p>
<b>keypair</b> for "cephadmin". You generate this from the Undercloud node using <b>"nova keypair-add cephadmin &gt; cephadmin.pem"</b>
</p>
<p>
<b>netid</b>: Execute <codeph>nova network-list</codeph> from the Undercloud to get the <codeph>netid</codeph>. For example: "823ad730-6c69-42e6-b455-8a8173ceae81"</p>
<p>You can verify that the <codeph>pem</codeph> file is present in Undercloud node by:</p>
<ol>
<li>
<p>Logging to the Undercloud.</p>

<codeblock>
<codeph># ssh heat-admin@&lt;undercloud IP address&gt;
</codeph>
</codeblock>
</li>
<li>
<p>Then enter.</p>

<codeblock>
<codeph>ls -ls
</codeph>
</codeblock>

<p>For example:</p>

<codeblock>
<codeph>root@undercloud-undercloud-zyjo2vylo3tb:~# ls -ls
total 12
4 -rw------- 1 root       root       1680 Feb 10 22:17 cephadmin.pem
4 -rw------- 1 heat-admin heat-admin  347 Feb 11 21:12 overcloud.stackrc
4   -rw-r--r-- 1 root       root        311 Feb 10 20:34 stackrc
</codeph>
</codeblock>
</li>
</ol>
</required-cleanup>
</section>
<section>
<title>Running the Provisioning Tool</title>
<p>To start the provisioning tool:</p>
<ol>
<li>Open a new terminal.</li>
<li>
<p>Log into the seed VM and launch a server from the <codeph>/helion-ceph/node-provisioner/server/' directory  using the</codeph>./server.py' script.</p>

<p>When you start the server, the following information is displayed.</p>

<codeblock>
<codeph>root@hLinux:~/helion-ceph/node-provisioner/server# ./server.py
Bottle v0.13-dev server starting up (using WSGIRefServer())...
Listening on http://192.168.51.22:8085/
Hit Ctrl-C to quit.
</codeph>
</codeblock>
</li>
<li>
<p>Open another seed terminal window and modify  the <codeph>orchestration.json</codeph>  file in <codeph>/helion-ceph/node-provisioner/client/orchestration.json</codeph>
</p>

<p>Your <codeph>baremetal.csv</codeph> inputs should match the flavor list. For example you need to specify the right RAM, vCPUs, disk size in "GB" and version number.</p>
</li>
</ol>
<ul>
<li>
<p>Update the  <codeph>"ws_url"</codeph> with the URL of the seed node VM where  the server is running by entering:</p>

<codeblock>
<codeph>"ws_url": "http://192.168.51.22:8085/"
</codeph>
</codeblock>
</li>
<li>
<p>Modify <codeph>imagepath</codeph> to include the path to images (deploy images + qcow + ramdisk + kernel) by entering:</p>

<codeblock>
<codeph>"imagepath": "/root/helion-ceph/images/"
</codeph>
</codeblock>

<p>For example, make changes to the following based on your setup.</p>

<codeblock>
<codeph>{
"authentication": {
       "ws_url": "http://192.168.51.22:8085/"
    },
    "api":{
       "imagepath": "/root/helion-ceph/images/",
       "deploy-image-prefix": "bm-deploy"
    },
    "orchestration":{
       "hypervisorsleepduration": "300",
       "hypervisorsmoniteringfrequency": "10",
       "bootsleepduration": "1200",
       "bootinitialwaitduration": "30",
       "hypervisortype": "baremetal",
      "hypervisordriver": "ironic",
       "bootmoniteringfrequency": "5",
       "destinationpath": "/root/helion-ceph/"
    },
    "flavor": {
           "001": {
               "ram": "163840",
               "vcpus": "2",
               "disk": "275",
               "architecture": "x86_64",
               "version" : "001"
          },
          "002": {
               "ram": "163840",
               "vcpus": "2",
               "disk": "200",
               "architecture": "x86_64",
               "version" : "001"
           }
    }
</codeph>
</codeblock>

<p>6.Modify the <codeph>baremetal.csv</codeph> file in <codeph>/helion-ceph/node-provisioner/client/</codeph> to include the details of the node. The following is an example:</p>

<codeblock>
<codeph>f0:92:1c:05:db:48,helioncsel,m0ng00s3,10.1.67.134,2,163840,200,overcloud-ceph-rados,mon2,002
&lt;mac address&gt;, &lt;ipmiusername&gt;, &lt;impipassword&gt;, &lt;ipaddress&gt;, &lt;cpu&gt;, &lt;memory&gt;, &lt;disksize&gt;, &lt;imagename&gt;, &lt;node name&gt;, &lt;flavor ID&gt;
f0:92:1c:05:57:30,helioncsel,m0ng00s3,10.1.67.123,2,163840,200,overcloud-ceph-cluster,c1mon1,002
f0:92:1c:05:bb:b0,helioncsel,m0ng00s3,10.1.67.124,2,163840,200,overcloud-ceph-cluster,c1mon2,002
f0:92:1c:05:4c:10,helioncsel,m0ng00s3,10.1.67.125,2,163840,200,overcloud-ceph-cluster,c1mon3,002
f0:92:1c:05:6c:78,helioncsel,m0ng00s3,10.1.67.126,2,163840,200,overcloud-ceph-cluster,c1gw1,002
f0:92:1c:05:ac:30,helioncsel,m0ng00s3,10.1.67.127,2,163840,200,overcloud-ceph-cluster,c1gw2,002
f0:92:1c:05:aa:d8,helioncsel,m0ng00s3,10.1.67.128,2,163840,200,overcloud-ceph-cluster,c1osd1,002
9c:b6:54:97:44:10,helioncsel,m0ng00s3,10.1.67.129,2,163840,200,overcloud-ceph-cluster,c1osd2,002
f0:92:1c:05:7c:58,helioncsel,m0ng00s3,10.1.67.130,2,163840,200,overcloud-ceph-cluster,c1osd3,002
9c:b6:54:97:a3:e0,helioncsel,m0ng00s3,10.1.67.131,2,163840,200,overcloud-ceph-admin,c1admin,002
</codeph>
</codeblock>

<p>Note that the flavor definition (<codeph>flavor ID</codeph>) in  the <codeph>baremetal.csv</codeph> file should match the flavor defined in <codeph>orchestration.json</codeph>.</p>
</li>
</ul>
<p>7.Open another seed terminal window  and launch the client <codeph>python orchestration.py</codeph> that is on the seed node VM in <codeph>/helion-ceph/node-provisioner/client/</codeph>
</p>
<p>8.To monitor this process, open a new terminal seed window and run</p>
<codeblock>
  <codeph>`tail -f -50 /client/orchestration.log`
</codeph>
</codeblock>
</section>
<section>
<title>Verifying your Instance</title>
<p>The provisioning tool takes about 10-15 minutes to provision a single baremetal node.</p>
<p>Check the <codeph>nova list</codeph> to see if the instance goes from spawning  to active state. You should be able to SSH to the instance after 12 minutes once you run the script.</p>
<p>If the instance does not spawn, check:</p>
<ul>
<li>The flavor definition matches the actual machine hardware configuration</li>
<li>Check for any errors in <codeph>orchestration.log</codeph>
</li>
<li>Verify entries in <codeph>server.json</codeph> and <codeph>orchestration.json</codeph>
</li>
<li>Check for any errors in the server terminal window</li>
<li>Check that all the images are loaded to Glance</li>
<li>Make sure that the instance you are tying to provision does not exist in <codeph>ironic node-list</codeph> or <codeph>nova list</codeph>
</li>
<li>Check for any other Nova-related errors in <codeph>/var/log/nova/nova-compute.log or /var.log/ironic/ dir</codeph>
</li>
</ul>
<p>Once all your instances are active and up, move on to the next step.</p>
<p>SSH to your newly launched instance using the <codeph>cephadmin</codeph> key that was generated from the Undercloud in the previous steps.</p>
<p>For example:</p>
<codeblock>
  <codeph>root@undercloud-undercloud-zyjo2vylo3tb:~# ssh -i cephadmin.pem hlinux@192.168.51.133
</codeph>
</codeblock>
<p>The programs included with the hLinux system are free software. For information on specific
licensing terms for each program, see the individual files in
<codeph>/usr/share/doc/*/copyright</codeph>.</p>
<p>When you SSH into your instance, you should see (for example):</p>
<codeblock>
  <codeph>Last login: Thu Feb 12 04:33:42 2015 from 192.168.51.23
hlinux@admin-overcloud-ceph-admin:~$ 
</codeph>
</codeblock>
</section>
<section>
<title>Provisioning Additional Ceph Nodes</title>
<p>To add new nodes, create new <codeph>baremetal.csv</codeph> entries and follow the above provisioning steps.</p>
<p>
<b>Note</b>:  Do not append new entries to existing entries in  the <codeph>baremetal.csv</codeph> file or else the existing nodes will be reprovisioned.</p>
<!-- ===================== horizontal rule ===================== -->
<p>
  <xref href="#topic9683/top"> Return to Top ↑ </xref>
</p>
</section>
</body>
</topic>
